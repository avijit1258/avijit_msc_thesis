% TO DO LIST Background
% LDA
% Strike a match
% Pyramid score
% Sequential pattern mining
% abstract code summary tree bring from later chapter and enhance
% Background related works for committee

\label{chapter:background}

In this chapter, we briefly discuss relevant terms, topics and techniques helpful to this thesis. In Section \ref{background:call_graph}, we elaborate terms relevant to a call graph. We then present an  abstract code summary tree in Section \ref{background:cct}. In Section \ref{background:motive}, we provide an abstract code summary tree for a sample calculator program using our system. In Section \ref{background:techniques}, we have elaborated different techniques and algorithms used in the thesis. In Section \ref{background:related_work}, we discuss related work for the studies done in the thesis.  

\section{Call graph}
\label{background:call_graph}
A \emph{call graph} is a control flow graph of a program showing calling relationships between functions. Each node of the graph represents a function and each edge $(a, b)$ represent calling relationship where function $a$ calls function $b$. Figure \ref{fig:bg_call_graph} shows a simple call graph with six nodes indicating functions and six edges indicating calling relationships. Call graphs can be of two types. One type is a static call graph. A static call graph contains all the possible program execution scenarios. To generate a static call graph, source code of the program is analyzed to find the relationships. A dynamic call graph represents one program run scenario. Therefore, a dynamic call graph is exact and limited to the scenarios used to generate the graph. To generate a dynamic call graph, logger or profiler is applied which generates call graph during run-time of the program.

\begin{figure}[h]	
	\centering
	\begin{subfigure}[h]{3in}
	\includegraphics[width=3in]{figures/background/call graph.png}
	\caption{A sample call graph}\label{fig:bg_call_graph}		
	\end{subfigure}
	\quad
	\begin{subfigure}[h]{3in}
		\includegraphics[width=3in]{figures/background/execution_paths.png}
		\caption{All execution paths from the call graph}\label{fig:bg_execution_path}
	\end{subfigure}
	\caption{Call graph with entry node, exit node and execution paths}\label{fig:1}
\end{figure}

An \emph{entry node} for a call graph is the node in which the number of incoming degrees is zero. In Figure \ref{fig:bg_call_graph}, the call graph has two entry nodes \emph{F0, F3}. No other nodes call the functions or nodes \emph{F0, F3}. That means program execution can start from these nodes.

An \emph{exit node} for a call graph is the node in which number of outgoing degrees is zero. In Figure \ref{fig:bg_call_graph}, the call graph has two exit nodes \emph{F0, F3}. The exit nodes \emph{F0, F3} do not call any other functions or nodes. That means program execution will end when we come to these nodes.


The \emph{execution paths} of a call graph are the all possible program execution scenarios. A program execution scenario consist of a function call sequence starting from a \emph{entry node} and ending to a \emph{exit node} of the call graph. In Figure \ref{fig:bg_execution_path}, all the execution paths from the call graph of Figure \ref{fig:bg_call_graph} are listed. The first node of the execution paths are the Entry nodes which is defined above. Similarly, the last node of the execution paths are the Exit nodes. 




\section{Abstract Code Summary Tree}
\label{background:cct}
In this thesis, we introduce a term called abstract code summary (ACS) tree. In an ACS tree each leaf node is attached to an execution path extracted from the call graph of a software system. The parent nodes of the leaf nodes are grouping of similar execution paths (leaf nodes). We call this intermediate nodes an abstraction node as it abstracts similar execution scenarios. Each abstraction node has three properties which are title, text summary and execution patterns. 

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figures/background/cct.png}
  \caption{An abstract code summary tree with its different components }
  \label{background:cct}
\end{figure}
In Figure \ref{background:cct}, we present a ACS tree where \emph{4, 5, 6, 7} nodes are leaf node which are attached to execution paths. Nodes \emph{1, 2, 3} are abstraction nodes which are grouping of the leaf nodes. Each abstraction nodes has number of execution paths and we use different information from those execution paths to generate concepts for them. Node 3 has two execution paths which belong to node 6 and 7. Like all other abstraction nodes Node 3 will have title, text summary and execution patterns. The title of Node 3 will be generated using different information retrieval techniques utilizing method signatures. Next, the text summary of Node 3 will be generated by summarizing comments of the methods which belong to the execution paths of Node 3. The execution patterns for Node 3 will be generated by finding frequent patterns from the execution paths of Node 3.

\section{Motivational Example}
\label{background:motive}
To demonstrate how a software system's hierarchical abstraction will work, we have created a sample Calculator program. The program takes two numbers as inputs, validates the inputs, and prompts the user to input which operations they want to perform. Later, according to the input, addition, subtraction, multiplication, division can be performed. This is a brief functionality of the calculator program. We have provided the source code of the Calculator program in appendix \ref{appendix:calculator}.

In Figure \ref{fig:motivation}, we have presented the hierarchical abstraction of the Calculator program. From the figure, we can see our Calculator program has six execution paths. Their node numbers are from 0-5. 

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figures/hla2/hla2_motivation.png}
  \caption{An abstract code summary of the calculator program (EP means Execution path or leaf node and AN means Abstraction Node)}~\label{fig:motivation}
\end{figure}

\textbf{Constructing the abstract code summary.} To generate the tree shown in Figure \ref{fig:motivation}, the following steps are followed.

\begin{enumerate}
    \item To get the caller-callee relationships from the source code of Calculator program, we use a static source code analyzer.
    \item We construct a static call graph from the extracted relationships of \emph{Calculator.py} program. 
    \item From the call graph, possible execution scenarios are generated  which are the execution paths shown in Figure \ref{fig:motivation} (EP 0 - 5).
    \item Similarity scores for each pair of execution paths are calculated which is used by the clustering algorithm to group the execution paths. As EP 0, 1, ...., 4 all have three common functions, the similarity measure between them  will be same.  
    \item A clustering algorithm starts grouping the execution paths by taking the most similar two first. In Figure \ref{fig:motivation}, we see that EP 0, 1 are grouped together as abstraction node (AN) 7.
    \item As AN 7 have EP 0, 1, we use information retrieval techniques on all the terms in functions names of EP 0, 1 to label the node AN 7. 
    \item Although keywords are helpful for providing hints to features, having a text description and frequent execution patterns for each abstraction node increases comprehension. In Table \ref{table:node_summary_patterns}, we presented node summary and execution patterns for AN 10, 11. 
    
\end{enumerate}



\begin{table*}[h]
\caption{Abstraction Nodes with summary and execution patterns}
    \centering
    \begin{tabular}{ |l | p{5cm} | p{10cm} | } 
 \hline
 AN & Node Summary & Execution Patterns \\ 
 \hline
 11 & This function multiplies two numbers. This function mod two numbers. This function subtract two numbers.  &  \bullet init \rightarrow two\_number\_input \rightarrow valid\_number. 
 \bullet init \rightarrow operations\_to\_do \rightarrow add\_two\_numbers. 
 \bullet init \rightarrow operations\_to\_do \rightarrow divide\_two\_numbers. 
\bullet init \rightarrow operations\_to\_do \rightarrow mod\_two\_numbers. 
 \bullet init \rightarrow operations\_to\_do \rightarrow multiply\_two\_numbers. 
 \bullet init \rightarrow operations\_to\_do \rightarrow subtract\_two\_numbers. \\ 
10 & This function mod two numbers. This function divide two numbers. This function subtract two numbers. & 
\bullet init \rightarrow operations\_to\_do \rightarrow add\_two\_numbers. 
\bullet init \rightarrow operations\_to\_do \rightarrow divide\_two\_numbers. 
\bullet init \rightarrow operations\_to\_do \rightarrow mod\_two\_numbers. 
\bullet init \rightarrow operations\_to\_do \rightarrow multiply\_two\_numbers. 
\bullet init \rightarrow operations\_to\_do \rightarrow subtract\_two\_numbers. 
\\ 
 \hline
\end{tabular}
    \label{table:node_summary_patterns}
\end{table*}



\textbf{Exploring the abstract code summary.}
\begin{itemize}
    \item Execution path 0 and 1 represent the functionality of multiplying two numbers and adding two numbers, respectively. For these two clusters, add and multiply are the two different jobs they are doing. Other functions of the two paths are similar. So, the abstraction of these two execution paths is abstraction node 7. Five keywords are picked as the abstraction of execution paths 0 and 1. From the keywords of node 7, it is clear that descendent nodes do addition and multiply on two numbers.
    \item Next, for node 10, we can see the keywords are add, divide, mod, multiply, and subtract. These five keywords indicate that the descendant nodes of 10 do these numerical operations. If we observe the five execution paths (EP 0 - 4), we find that they perform add, delete, mod, multiply operation on two input numbers. We can see that the five keywords of node 10 summarize the functionality of its descendants.
    \item Similarly, for node 11, the keywords are mod, multiply, subtract, valid, and number. We can see the right child node (node 5) of node 11 input two numbers and then validates it. Left descendants of node 11 perform numerical operations. So, the summary of node 11 contains three words relevant to operation and two for input validation.
\end{itemize}
   From our understanding, we can see that this is an almost human level summary for node 10. The summary presented in Figure \ref{fig:motivation} is generated using TFIDF scores on words in method names. 

\section{ Techniques and Algorithms}
\label{background:techniques}
In this Section, we discuss important techniques and algorithms used to construct ACS tree. In Subsection \ref{background:tfidf}, \ref{background:lda} and \ref{background:lsi}, we discuss TFIDF, LDA and LSI technique which are used for generating node title from method names. In Subsection \ref{background:JD}, we discuss Jaccard Distance which is used to calculate similarity between execution paths. In Subsection \ref{background:AHC}, we discuss AHC algorithm which is used to cluster execution paths. Finally, we discuss Text Rank algorithm which generate node summary from method comments in Subsection \ref{background:text_rank}.

\subsection{TFIDF}
\label{background:tfidf}
TFIDF~\cite{ramos2003usingTfidfRelevance} is a weight based statistical information retrieval technique. It tries to find important terms to a specific document by analyzing a collection of documents. TFIDF is popular for document classification, search engine ranking and text mining\footnote{https://en.wikipedia.org/wiki/Tf–idf}. TFIDF ranks terms by term frequency-inverse document frequency score. Term frequency is the count of a term in a document. Term frequency is biased towards frequent terms which mostly stop words and other fairly meaningless words irrelevant to the document. 

\begin{equation}
    tf (W_x, D_x) = f_{W_x,D_x}
    \label{eq:tf_background}
\end{equation}
\begin{equation}
    idf(W_x) = \log(\frac{n}{df(W_x)})+1
    \label{eq:idf_background}
\end{equation}
\begin{equation}
    tf-idf(W_x, D_x) = tf(W_x,D_x) * idf(W_x)
    \label{eq:TFIDF_background}
\end{equation}


Jones~\cite{jones1972statistical} introduced inverse document frequency which penalties common terms by counting their occurrence across the corpus. Let, $D = \{D_1, D_2, ..., D_n\}$ is a collection of documents and $W = \{W_1, W_2, ....., W_n\}$ is unique terms in the collection of documents. Now, to calculate term frequency for term $W_x$ in document $D_x$, we have to count frequency of term $W_x$ in  document $D_x$ which is required to calculate term frequency according to equation \ref{eq:tf_background}. In addition, we have to count the number of documents has term $W_x$ which is used to calculate inverse document frequency using equation \ref{eq:idf_background}. In equation \ref{eq:idf_background}, $n$ is the number of documents in the corpus and $df(W_x)$ is the number of documents which contain term $W_x$. Equation \ref{eq:TFIDF_background}, multiplies term frequency and inverse document frequency to reward significant terms and penalize common terms. We have adopted \texttt{TFIDFVectorizer} class of scikit-learn \cite{scikit-learn} library for implementing $TFIDF$ technique.

\subsection{LDA}
\label{background:lda}
Latent Dirichlet Allocation (LDA)~\cite{blei2003latentLDA} is a statistical model that tries to describe a set of documents by assuming they are created from some topics. LDA is a very popular topic modeling technique. LDA assumes every term in a document belongs to some topic. So, it assumes each term belongs to some topic and then performs analysis to find which assumptions are supported by statistics of the corpus. We have used Gensim \cite{gensim} library for implementing LDA for our approach.

\subsection{LSI}
\label{background:lsi}
Latent Semantic Indexing (LSI)~\cite{deerwester1990indexingLSI} focuses on information retrieval based on semantic similarity between words where the previous techniques focus on matching words in query with words of documents. The semantic concept used in LSI assumes semantically similar words appear together. Information retrieval techniques which matches words suffer two limitations. They are \emph{synonymy} and \emph{polysemy}. \emph{synonymy} is the issue where the same object is described by different words depending on needs, knowledge and linguistic habits. On the other hand, \emph{polysemy} refers to the fact that words have multiple distinct meanings in different contexts. LSI, first, starts with a  Term-Document matrix where all terms are presented in the rows and documents in the columns. Table \ref{tb:LSI_term_document} shows an example of a Term-Document matrix. 

\begin{table}[h]
    \centering
    \caption{Sample Term-Document matrix}
 \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    
        & ship & boat & ocean & voyage & trip   \\
        \hline
        Document 1 & 1 & 0 & 1 & 0 & 0  \\
        Document 2 & 0 & 1 & 0 & 1 & 0    \\
        Document 3 & 1 & 0 & 0 & 1 & 1  \\
    \hline
    \end{tabular}
    
    \label{tb:LSI_term_document}
\end{table}


Single Value Decomposition (SVD) method is used to project the term-document matrix to reduced numnber of dimensions.
The reduced matrix by SVD is an approximation of the term-document matrix which is a representation of the semantic similarity between words in documents. If we need to find similarity between a query, the query is converted to similar representation and compared to find relevant documents. By using this technique, LSI can detect semantic similarity even when the terms are different. Similar to LDA, we used Gensim \cite{gensim} library for implementing LSI.   



\subsection{Jaccard Distance}
\label{background:JD}
Jaccard Distance can measure similarity between two sequences according to equation \ref{eq:jaccard}. For example, we have two execution path $E_i$ and $E_j$ and they have set of function names $F_i$ and $F_j$ respectively. Therefore, similarity between $E_i$ and $E_j$ can be measured by equation \ref{eq:jaccard}. 
\begin{equation}
\label{eq:jaccard_similar}
    JD\_similar(E_i, E_j) =  \frac{F_i \bigcap F_j}{F_i \bigcup F_j}
\end{equation}

\begin{equation}
\label{eq:jaccard_dissimilar}
    JD\_dissimilar(E_i, E_j) =  1 - \frac{F_i \bigcap F_j}{F_i \bigcup F_j}
\end{equation}
If $E_i$ and $E_j$ are very similar, according to equation \ref{eq:jaccard_similar} similarity score will be near 1 and vice-versa. Clustering algorithm merges those two clusters which distance measures are minimum. Equation \ref{eq:jaccard_dissimilar} subtract Jaccard Distance by 1 to get desire dissimilarity measure for clustering algorithms.

\subsection{Agglomerative Hierarchical Clustering}
\label{background:AHC}
Clustering algorithms are popular in many data mining, unsupervised machine learning and pattern recognition applications. Clustering algorithms try to group similar observations together to find significant patterns in the observations. Hierarchical clustering can be done in two ways. One is bottom-up (agglomerative) and another is top-down (divisive). For divisive clustering, all observations starts in a single cluster and divided into different clusters using heuristics. Agglomerative clustering starts by considering observations as individual clusters and then group them until all observations end-up in the same cluster.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.5\columnwidth, height=0.5\columnwidth]{figures/background/agglomerative_clustering.png}
  \caption{Agglomerative and Divisive clustering algorithm with a sample cluster forest}~\label{fig:agglomerative_clustering}
\end{figure*}
In Figure \ref{fig:agglomerative_clustering}, a visualization of how agglomerative and divisive clustering algorithm works are presented. Lets assume there are five observations \emph{a, b, c, d, e} and we have similarity score between all the pairs of the observations. First, we can see five observations are treated as five clusters. From the similarity score we found that clusters \emph{d and e} are most similar. Therefore, we group cluster \emph{d and e} together as a new cluster \emph{de}. Now, in the cluster forest we have four clusters. In the next step, cluster b and c are the most similar. So, agglomerative clustering algorithm will group cluster b and c as a new cluster \emph{bc}. The agglomerative clustering will continue to merge clusters together until there is only one cluster in the cluster forest. For this example, the final cluster \emph{(abcde)} consists of all the initial clusters. 

\subsection{Text Rank}
\label{background:text_rank}
Mihalcea \cite{mihalcea2004textrank} proposed a graph based ranking algorithm called TextRank inspired by the PageRank algorithm to rank entities in natural language. Two of the significant application of TextRank are keyword extraction and sentence extraction. Sentence extraction can be formulated to generate summary of natural language text. To generate a summary of a paragraph, first, sentences are split as they are the unit for TextRank algorithm. Next, sentences are converted to word embedding vectors. In the next step, similarity matrix is computed from embedding vectors. Then, a graph is created where vertices are sentences and edges represent similarity scores between sentences\footnote{https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/}. Similarity scores are used to extract top ranked sentences according to equation \ref{eq:textrank}.

\begin{equation}
\label{eq:textrank}
    WS(V_i) = (1 - d) + d * \sum_{V_j\epsilon IN(V_i) } \frac{w_{ji}}{\sum_{V_k \epsilon Out(V_j)} w_{jk}}  WS(V_j)
\end{equation}

Let, $ G = (V, E)$ is a directed graph where V is the collection of vertices and E is the collection of edges. $In(V_i)$ is the set of vertices which points to vertex $V_i$. Similarly, $Out(V_j)$ is the set of vertices which vertex $V_j$ points to. The similarity score between vertex $V_i$ and $V_j$ is represented by $w_{ji}$. 


\section{Related work}
\label{background:related_work}
\subsection{Program Comprehension in General}
\label{related:program_comprehension}
Program comprehension is a cognitive way of understanding software systems to perform different software maintenance tasks \cite{wei2002surveyCategorizationComprehension, siegmund2016programPastFuture}. Three different types of cognitive models \cite{tilley1998reverseEngineeringFramework, von1993programToolRequirements, siegmund2016programPastFuture} can be found in the literature which is followed consciously or unconsciously by developers. The comprehension models are Top-down, Bottom-up, and Integrated. When developers have prior domain knowledge about a software system, the top-down model is preferred as they can map domain knowledge to low-level source code hierarchically \cite{brooks1983theoryComprehensionPrograms}. On the other hand, when developers lack domain knowledge, they start with low-level source code and then group the functionality together to have a hierarchical abstraction of the system features \cite{shneiderman1979syntacticInteractionsModel, pennington1987stimulusMentalRepresentations}. Integrated model \cite{shaft1995relevanceDomainKnowledge, von1993programToolRequirements} is a mix of top-down and bottom-up approaches. The problem in hand and the target system have different properties in the real world, which demand switching between top-down and bottom-up models. Generally, a developer can have prior domain knowledge of a few portion and point-blank for the rest of the system. This situation deserves the adapted use of both top-down and bottom-up approaches.   


\subsection{IR Techniques to Name Source Code Artifacts}
\label{related:IR}
% \cite{mcburney2014improvingTopicSummarize}
% \cite{de2012IRMethodsArtifacts} \cite{panichella2013topicModelsTasks}
% \cite{chen2016topicMiningRepositories}
% Very very important \cite{sun2016surveyTopicSE}
As software repositories contain unstructured data, topic model techniques are widely applied for different software engineering tasks to retrieve information \cite{chen2016topicMiningRepositories, panichella2013topicModelsTasks, sun2016surveyTopicSE}. Most common tasks where topic models showed promising results are source code comprehension, feature location, refactoring, bug localization, and others \cite{sun2016surveyTopicSE}. Lucia et al. \cite{de2012IRMethodsArtifacts} conducted a study to see how information retrieval techniques perform compared to manual naming Java class files. Developers were asked to pick ten keywords for each class file, and top-10 words are picked using different topic model technique and custom heuristics. Their experiment shows that in 40\%-80\% cases, automatic and human labels overlap. 

\subsection{Reverse Engineering}
\label{related:reverse_engineering}
\subsubsection{Subsystem Identification}
Muller et al. \cite{muller1990composingSubsystemStructures} proposed subsystem detection algorithm using different clustering components like variable, procedure, and modules. 
According to Bass et al. \cite{bass2003softwareArchitecturePractice}, two types of software architecture are useful for understanding a complex software system. They are Conceptual and Concrete architecture. A conceptual architecture provides high-level abstraction skipping the code level details. On the other hand, concrete architecture shows the implementation level information. Roy et al. \cite{roy2008softwareArchitectureRecovery} propose and evaluate a framework for the incremental and iterative application of automated architecture recovery (using SWAG Kit) and architecture analysis (using SAAM.). They showed that the reverse engineering tool cannot recover a deeply understood conceptual architecture without SAAM's application but can create a reasonable basis towards that direction. Murphy et al.\cite{MurphyNotkin2001} show that by generating reflexion models from high-level model and source model (i.e., static call graphs), it is possible to facilitate program understanding to the novice developers. In this thesis, we try to automatically recover conceptual architecture from concrete architecture, reducing manual effort.

\subsubsection{Call Graphs to Abstract a Software System Behaviors}

Static and dynamic call graphs are used in literature to help developers comprehend a software system to aid different maintenance tasks \cite{feng2018hierarchicalExecutionComprehension, gharibi2018automaticStaticCluster, xin2019identifyingFeaturesExecution}. Feng et al. \cite{feng2018hierarchicalExecutionComprehension} proposed an approach to use dynamic call graphs for understanding a system's behavior. They instrumented the subject systems to generate execution traces of method entry and exit events. Later, they followed the duplication removal process and constructed a call graph from the execution traces. Execution phases from this dynamic call graph are clustered to get system behaviors. Similarly, Gharib et al. \cite{gharibi2018automaticStaticCluster}, and Vijay et al. \cite{walunj2019graphevoEvolutionCall} also adopted clustering of execution paths from call graphs of the static variant. Using a static call graph brings the benefit of capturing all possible scenarios and less redundant data to handle than dynamic call graph \cite{gharibi2018automaticStaticCluster}. 

\subsubsection{IR Techniques on the Hierarchical Abstraction of a Software System}
Feng et al. \cite{feng2018hierarchicalExecutionComprehension} proposed an approach to identify behaviors of a system by hierarchically abstracting dynamic call graph from execution traces. Sequential pattern mining is applied to mine significant portions from the execution phases. Hierarchical clustering is performed to group execution phases. Later, the clusters are labeled using the TFIDF score, where method signatures serve as terms and the phases as document. 
Paul et al. \cite{mcburney2014improvingTopicSummarize} used static call graph to hierarchically abstract a system. In their hierarchical view, each node represents a method. To mine the topics, keywords from methods are considered. Hierarchical Document Topic Model (HDTM) by \cite{weninger2012documentTopicHierarchies} Weninger et al. is adopted, which works on graph documents to mine topic. Gharib et al. \cite{gharibi2018automaticStaticCluster} took a different approach. They went further with the static call graph by extracting execution paths and then clustering the execution paths. Each cluster in the cluster tree is labeled using top-5 method names from Tfidf. Levy et al. \cite{levy2019understandingLargeHierarchical} found interviewing developers that two kinds of comprehension go for large scale hierarchical view. They are system comprehension and code comprehension. In this thesis, we tried to adopt static call graph analysis from Gharib et al. and then improve their labeling technique. Nodes of the cluster tree is considered as a behavioral abstraction unit of a system. Method comments are used to generate a description of the unit and sequential pattern mining to create sample examples. 


\subsection{How Developers Locate Features in Source Code}
\label{related:feature_locate}
Kruger et al.~\cite{kruger2019features} studied two data sets (67 developers IDE activity, 600 developers IR-based tool usage). They suggested that there is room for improvement in the existing code navigation, code search tools. The manual processes followed by developers to locate features are of mostly three types~\cite{damevski2016field, wang2011exploratory, revelle2005understanding}. First, developers use information retrieval based tools to query for feature related keywords. In this thesis, we have used IR based techniques to label nodes. Developers can use our tool to find keywords of their interest. Second, there is an  execution-based process where developers try to find execution scenarios where the feature is active. After finding relevant execution scenarios, developers debug the execution scenarios by setting breakpoints. In our second study, we have attached execution patterns to nodes which can be utilized by developers to know where to set the breakpoints for understanding a feature. Third, there is an exploration-based process where developers explore source code to understand method calls to find a feature. In the HCPC tool, we showed method execution patterns for each node. Our tool can also help developers in browsing code using an  exploration-based process.

\subsection{Program Comprehension with Static and Dynamic Call Graph }
Feng \textit{et al.} \cite{feng2018hierarchicalExecutionComprehension} proposed an approach to abstract execution traces for program comprehension. To get execution traces, they used BLINKY to instrument source code for getting method-invocation calls. Different test cases are used to generate execution traces for different scenarios. From dynamic logs, they have built phase trees that are created from caller-callee relationships of invoked methods. After deleting duplicate phases, they clustered unique phases using the Agglomerative hierarchical clustering algorithm. Next, they applied a mining technique to get frequent pattern phases of each level of clustered phase tree. For comprehension purposes, they used TFIDF to rank method names of frequent phases and then used the top 20 method names for the final label. Depending on dynamic call graphs comes with some limitations as it depends on the test cases heavily, and the size of log file generated is difficult to handle. Therefore, we choose static call graphs to remove the test dependency and capture a call graph's overall execution scenario. Gharib \textit{et al.}  \cite{gharibi2018automaticStaticCluster} proposed an approach using static call graphs for hierarchical abstraction. First, they generated a static call graph for a subject system that captures overall function relationships. Second, execution paths from the call graph are extracted, which become the building blocks for their approach. Next, execution paths are clustered together to create abstract code summary of the target subject systems. Feng \textit{et al.}~\cite{feng2018hierarchicalExecutionComprehension} also named the clusters by extracting keywords from the function names present in execution paths. In their study, only the TFIDF technique is applied to extract and name intermediate clusters.

For this study, our motivation is to take forward this approach and enrich it with existing techniques from the literature. Two limitations of the study from Gharib \textit{et al.}  are using only TFIDF method for information retrieval and no presence of user study to validate how developers prefer the output abstractions. We adopt two more topic modeling techniques for information retrieval, which show promising results for naming source code artifacts in the literature \cite{de2012IRMethodsArtifacts}. Andrea \textit{et al.}  \cite{de2012IRMethodsArtifacts} tried to apply IR techniques like VSM, LDA, and LSI on  source code artifacts. To evaluate IR techniques' effectiveness, they also produced suggestions from 17 users on the same classes. Then, they assessed the performance of automatic naming by comparing overlap
with manual naming of users. In their study, authors also find that heuristic based approaches focusing on function signatures perform well for code artifacts summarization. Inspired from their study, we use LDA and LSI on function signatures to extract concepts in code in this study. Another improvement from Gharib \textit{et al.} is to adopt a user study for validating automatic abstraction. Sonia \textit{et al.}  \cite{haiduc2010supporting} used Pyramid score to evaluate the output of automatic code summary with developers' summary. We also adopt this Pyramid score, which is widely used for the evaluation of natural language summaries.






